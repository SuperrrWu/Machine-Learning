SVM(支持向量机算法)

在深度学习出现之前，SVM被认为机器学习中近十几年来最成功，表现最好的算法

形象来说：两类画直线问题分类

算法特点：

    SVM寻求区分两类的超平面，使边际最大。

    如何选取边际最大的超平面（Max Margin Hyperplane）

    超平面到一侧最近点的距离等于到另一侧最近点的距离，两侧的两个超平面平行

    分为线性可区分(linear reparable)和线性不可区分(linear insparable)

一、线性可区分(linear reparable)

超平面可以定义：

    W*X+b=0
    W：weight vector,
    W={w1,w2,w3.....wn},n是特征值的个数

假设2维特征向量：X=（x1,x2）

把baise想成额外的W0：

超平面的方程为：

    W0+W1*X1+W2*X2=0

所有超平面右上方的点满足：
    
    W0+W1*X1+W2*X2>0
    
所有超平面左下方的点满足：
   
    W0+W1*X1+W2*X2<0
    
调整weight，使超平面定义边际的两边：

    W0+W1*X1+W2*X2>=1 for yi=+1
    W0+W1*X1+W2*X2<=-1 for yi=-1
    
综合两个式子：

    yi(W0+W1*X1+W2*X2)>=1,对所有i而言
    
所有落在超平面上的点叫做支持向量（support vector，建立超平面）

分界的超平面和H1,H2上的任意一点的距离为：
    
    1/w向量的范数（根号下所有w^2合）

所以，最大化边际距离为：
    
    2/w向量的范数
    
例子：
    
    from sklearn import svm
    x=[[2,0],[1,1],[2,3]]
    y=[0,0,1]
    clf=svm.SVC(kernel="linear")
    clf.fit(x,y)
    print(clf)
    print(clf.support_vectors_)#支持向量，超平面上的点
    print(clf.support_)#支持向量的index下标
    print(clf.n_support_)#每一个超平面有几个支持向量
    print(clf.predict([[2,0]]))#注意是[[]]形式
    
    
特点：
1.训练好的模型的算法复杂度是由支持向量的个数决定的，而不是由数据的维度决定的，所有SVM不太容易产生overfitting。
2.SVM训练出来的模型完全依赖于支持向量，即使去掉所有非支持向量的点去掉，一样的结果。
3.一个SVM如果训练得出的支持向量个数比较小，SVM训练出的模型比较容易泛化。

二、线性不可分情况(linearly inseparable case)

数据集无论如何在超平面内画一条线无法分开点

两个解决步骤：

1、利用一个非线性的映射把原数据集中的向量点转化到一个更高维度的空间中（2维转3维）

2、在这个高维度的空间中找一个线性的朝平面来根据线性可分的情况处理

视频演示：https://www.youtube.com/watch?v=3liCbRZPrZA

例子：

        3维输入向量：
                        X=(x1,x2,x3)
        转化到6维空间Z中去：
                        X1=x1,X2=x2,X3=x3,X4=x1^2,X5=x1*x2,X6=x2^2
                        
        新的决策超平面
                        d(Z)=WZ+b

由于内积的算法复杂度高，所以我们利用核函数来取代计算非线性映射函数的内积

以下核函数和非线性映射函数的内积相同

        K（Xi,yj）=Xi dot product yj
        
常用的核函数(kernel functions)
h度多项式核函数
K(xi,xj)=(xi*xj+1)^n
高斯径向基核函数
K(xi,xj)=e^(-abs(xi-xj)^2/2a^2)
S型核函数
K(xi,ji)=tanh(kXi*Xj-baise)

SVM可以解决多分类问题


