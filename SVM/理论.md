SVM(支持向量机算法)

在深度学习出现之前，SVM被认为机器学习中近十几年来最成功，表现最好的算法

形象来说：两类画直线问题分类

SVM寻求区分两类的超平面，使边际最大。

如何选取边际最大的超平面（Max Margin Hyperplane）

超平面到一侧最近点的距离等于到另一侧最近点的距离，两侧的两个超平面平行

分为线性可区分(linear reparable)和线性不可区分(linear insparable)

一、线性可区分(linear reparable)

超平面可以定义：

    W*X+b=0
    W：weight vector,
    W={w1,w2,w3.....wn},n是特征值的个数

假设2维特征向量：X=（x1,x2）

把baise想成额外的W0：

超平面的方程为：

    W0+W1*X1+W2*X2=0

所有超平面右上方的点满足：
    
    W0+W1*X1+W2*X2>0
    
所有超平面左下方的点满足：
   
    W0+W1*X1+W2*X2<0
    
调整weight，使超平面定义边际的两边：

    W0+W1*X1+W2*X2>=1 for yi=+1
    W0+W1*X1+W2*X2<=-1 for yi=-1
    
综合两个式子：

    yi(W0+W1*X1+W2*X2)>=1,对所有i而言
    
所有落在超平面上的点叫做支持向量（support vector，建立超平面）

分界的超平面和H1,H2上的任意一点的距离为：
    
    1/w向量的范数（根号下所有w^2合）

所以，最大化边际距离为：
    
    2/w向量的范数


