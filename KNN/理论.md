KNN（最临近规则分类）
分类（classification）算法
输入基于实例的学习(instance-based learning),懒惰学习(lazy learning)

假设有以下训练集：

                            电影名称          打斗次数          接吻次数         电影类型
                            califonia men       3               104            Romance
                            Dudes               2               100            Romance
                            longblade           101             10             Action
                            
为了判断位置实例的类别，以所有已知类别的实例作为参考
选择参数K（根据实验测试，k一般选奇数，胜负！）
计算位置实例与已知实例的距离
根据少数服从多数的投票法则(majority-voting)，让未知实例归类为k个最临近样本中最多数的类别。

关于距离的衡量方法：
euclidean distance
两点：
D=[(x2-x1)^2-(y2-y1)^2]^(1/2)
多点：
E(x,y)=[Σ(xi-yi)^2]^(1/2)

其他距离衡量：余弦（cos），曼哈顿距离（经过多少个街区）

算法优缺点：
1.优点：简单，易于理解，（k一般选奇数，胜负！）
2.缺点：需要大量空间存储所有已知实例
算法复杂度高（需要比较所有已知实例与要分类的实例）
当其样本分布不平均时，比如其中一类样本过大（实例数量过多）占主导的时候，新的未知实例容易被归类为这个主导样本，因为这类样本实例的数量过大，但这个新的未知实例并未接近目标样本。

改进：考虑距离，根据距离加上权重
比如：1/d（d:距离，离得越近，权重越大）
